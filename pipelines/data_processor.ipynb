{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02e7e86b-52ff-4c41-bcfa-bcdfbc51cab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/Cellar/jupyterlab/4.5.3/libexec/lib/python3.14/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=2.3.3 in /opt/homebrew/Cellar/jupyterlab/4.5.3/libexec/lib/python3.14/site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.5.3/libexec/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.5.3/libexec/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting neo4j\n",
      "  Downloading neo4j-6.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pytz (from neo4j)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Downloading neo4j-6.1.0-py3-none-any.whl (325 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, neo4j\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [neo4j]â”â”â”â”â”\u001b[0m \u001b[32m1/2\u001b[0m [neo4j]\n",
      "\u001b[1A\u001b[2KSuccessfully installed neo4j-6.1.0 pytz-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# The '%' tells Jupyter to install it in the specific environment the notebook is using\n",
    "%pip install pandas\n",
    "%pip install neo4j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07746014-41e7-41d4-b43c-129c8da03e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of ../data/:\n",
      "['UpdatedResumeDataSet.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# This lists everything in your data folder so we can see the exact names\n",
    "data_path = \"../data/resume/\"\n",
    "if os.path.exists(data_path):\n",
    "    print(\"Contents of ../data/:\")\n",
    "    print(os.listdir(data_path))\n",
    "else:\n",
    "    print(\"The path '../data/' was not found. Are you sure you are in the 'pipelines' folder?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d13fc00a-dc9b-445f-8776-3af6bb9e1595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Loading ESCO and Resume data...\n",
      "ðŸ” Matching 962 resumes against 13939 ESCO skills...\n",
      "ðŸ“„ Exporting files to ../output/...\n",
      "âœ… Done! Files ready in /output folder.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- PATH CONFIGURATION ---\n",
    "# Based on your directory structure: intelligent-recruiter/pipelines/ -> ../data/\n",
    "BASE_DATA_PATH = \"../data/skill_data/\"\n",
    "RESUME_PATH = \"../data/resume/UpdatedResumeDataSet.csv\" # Confirm your filename in /resume\n",
    "OUTPUT_PATH = \"../output/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "def run_pipeline():\n",
    "    print(\"ðŸš€ Loading ESCO and Resume data...\")\n",
    "    \n",
    "    # 1. Load Files using your specific folder names\n",
    "    skills = pd.read_csv(f\"{BASE_DATA_PATH}skills_en.csv\")\n",
    "    occs = pd.read_csv(f\"{BASE_DATA_PATH}occupations_en.csv\")\n",
    "    rels = pd.read_csv(f\"{BASE_DATA_PATH}occupationSkillRelations_en.csv\")\n",
    "    resumes = pd.read_csv(RESUME_PATH)\n",
    "    \n",
    "    # 2. Build Skill Dictionary for matching\n",
    "    # ESCO provides unique URIs for every skill\n",
    "    skill_list = skills['preferredLabel'].dropna().unique().tolist()\n",
    "    \n",
    "    print(f\"ðŸ” Matching {len(resumes)} resumes against {len(skill_list)} ESCO skills...\")\n",
    "\n",
    "    # Accurate matching with word boundaries to avoid partial matches\n",
    "    def extract_skills(text):\n",
    "        text = str(text).lower()\n",
    "        found = []\n",
    "        for skill in skill_list:\n",
    "            skill_clean = skill.lower()\n",
    "            if re.search(r'\\b' + re.escape(skill_clean) + r'\\b', text):\n",
    "                found.append(skill_clean)\n",
    "        return list(set(found))\n",
    "\n",
    "    # Apply extraction\n",
    "    resumes['matched_skills'] = resumes['Resume'].apply(extract_skills)\n",
    "\n",
    "    # 3. Export CSVs for Neo4j Ingestion\n",
    "    print(f\"ðŸ“„ Exporting files to {OUTPUT_PATH}...\")\n",
    "    \n",
    "    # Nodes\n",
    "    skills[['conceptUri', 'preferredLabel']].to_csv(f\"{OUTPUT_PATH}nodes_skills.csv\", index=False)\n",
    "    occs[['conceptUri', 'preferredLabel']].to_csv(f\"{OUTPUT_PATH}nodes_occupations.csv\", index=False)\n",
    "    resumes[['Category']].to_csv(f\"{OUTPUT_PATH}nodes_candidates.csv\", index_label='candidate_id')\n",
    "\n",
    "    # Edges: Occupation -> REQUIRES -> Skill\n",
    "    # relationType identifies if a skill is essential or optional\n",
    "    rels[['occupationUri', 'skillUri', 'relationType']].to_csv(f\"{OUTPUT_PATH}edges_occ_skill.csv\", index=False)\n",
    "\n",
    "    # Edges: Candidate -> HAS_SKILL -> Skill\n",
    "    skill_to_uri = dict(zip(skills['preferredLabel'].str.lower(), skills['conceptUri']))\n",
    "    candidate_skill_edges = []\n",
    "    for idx, row in resumes.iterrows():\n",
    "        for s_name in row['matched_skills']:\n",
    "            uri = skill_to_uri.get(s_name)\n",
    "            if uri:\n",
    "                candidate_skill_edges.append({'candidate_id': idx, 'skillUri': uri})\n",
    "    \n",
    "    pd.DataFrame(candidate_skill_edges).to_csv(f\"{OUTPUT_PATH}edges_candidate_skill.csv\", index=False)\n",
    "    print(\"âœ… Done! Files ready in /output folder.\")\n",
    "\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72aba60-9847-469c-90bb-4594da9866d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning existing data...\n",
      "ðŸ“¥ Loading Skill Nodes...\n",
      "ðŸ“¥ Loading Occupation Nodes...\n",
      "ðŸ“¥ Loading Candidate Nodes...\n",
      "ðŸ”— Linking Candidates to Skills...\n",
      "ðŸ”— Linking Occupations to Skills (Taxonomy)...\n",
      "âœ… Neo4j Ingestion Complete!\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "\n",
    "# Connection details\n",
    "URI = \"neo4j://127.0.0.1:7687\"\n",
    "USER = \"neo4j\"\n",
    "PASSWORD = \"12345678\"\n",
    "\n",
    "def load_data_to_neo4j():\n",
    "    driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        print(\"ðŸ§¹ Cleaning existing data...\")\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\") # Optional: Clears graph before loading\n",
    "\n",
    "        # 1. Load Skills\n",
    "        print(\"ðŸ“¥ Loading Skill Nodes...\")\n",
    "        skills_df = pd.read_csv(\"../output/nodes_skills.csv\")\n",
    "        session.run(\"\"\"\n",
    "            UNWIND $data AS row\n",
    "            MERGE (s:Skill {uri: row.conceptUri})\n",
    "            SET s.name = row.preferredLabel\n",
    "        \"\"\", data=skills_df.to_dict('records'))\n",
    "\n",
    "        # 2. Load Occupations\n",
    "        print(\"ðŸ“¥ Loading Occupation Nodes...\")\n",
    "        occs_df = pd.read_csv(\"../output/nodes_occupations.csv\")\n",
    "        session.run(\"\"\"\n",
    "            UNWIND $data AS row\n",
    "            MERGE (o:Occupation {uri: row.conceptUri})\n",
    "            SET o.name = row.preferredLabel\n",
    "        \"\"\", data=occs_df.to_dict('records'))\n",
    "\n",
    "        # 3. Load Candidates\n",
    "        print(\"ðŸ“¥ Loading Candidate Nodes...\")\n",
    "        cands_df = pd.read_csv(\"../output/nodes_candidates.csv\")\n",
    "        session.run(\"\"\"\n",
    "            UNWIND $data AS row\n",
    "            MERGE (c:Candidate {id: row.candidate_id})\n",
    "            SET c.category = row.Category\n",
    "        \"\"\", data=cands_df.to_dict('records'))\n",
    "\n",
    "        # 4. Load Candidate -> Skill Relationships\n",
    "        print(\"ðŸ”— Linking Candidates to Skills...\")\n",
    "        cand_skill_df = pd.read_csv(\"../output/edges_candidate_skill.csv\")\n",
    "        session.run(\"\"\"\n",
    "            UNWIND $data AS row\n",
    "            MATCH (c:Candidate {id: row.candidate_id})\n",
    "            MATCH (s:Skill {uri: row.skillUri})\n",
    "            MERGE (c)-[:HAS_SKILL]->(s)\n",
    "        \"\"\", data=cand_skill_df.to_dict('records'))\n",
    "\n",
    "        # 5. Load Occupation -> Skill Relationships (The ESCO Taxonomy)\n",
    "        print(\"ðŸ”— Linking Occupations to Skills (Taxonomy)...\")\n",
    "        occ_skill_df = pd.read_csv(\"../output/edges_occ_skill.csv\")\n",
    "        session.run(\"\"\"\n",
    "            UNWIND $data AS row\n",
    "            MATCH (o:Occupation {uri: row.occupationUri})\n",
    "            MATCH (s:Skill {uri: row.skillUri})\n",
    "            MERGE (o)-[r:REQUIRES]->(s)\n",
    "            SET r.type = row.relationType\n",
    "        \"\"\", data=occ_skill_df.to_dict('records'))\n",
    "\n",
    "    driver.close()\n",
    "    print(\"âœ… Neo4j Ingestion Complete!\")\n",
    "\n",
    "load_data_to_neo4j()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90ca07-9866-4b2d-b324-a62e2ceb17a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
